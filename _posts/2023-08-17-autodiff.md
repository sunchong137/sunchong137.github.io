---
title: 'Automatic Differentiation'
date: 2023-08-17
permalink: /posts/2023/0817
author_profile: false
tags:
  - Neural Networks
  - Machine Learning
  - Python3
---

This post explains how forward- and reverse-mode automatic differentiation algorithms work with the Wikipedia example $f(x_1, x_2) = x_1 * x_2 + \sin(x_1)$ and Python implementations. Autodiff is conceptually easy to understand, but untrivial to implement. Therefore, unless you can write your own autodiff code, you cannot say confidently that you understand how it works. 

If you have some basic understanding of autodiff, you can skip my writing and jump to the implementation.

The first step is to divide $f$ into operation units labeled by $w^{n}_i$, where $n$ stands for hierarchy, and $i$ stands for the index of the members in the same hierarchy. The lowest hierarcy corresponds to the variables, i.e., $x_1$ and $x_2$, and the highest hierarchy corresponds to the function, i.e., $f$. Therefore, we have

$$
w^2 = w^1_1 + w^1_2;\\
w^1_1 = w^0_1 * w^0_2, \quad w^1_2 = \sin(w^0_1); \\
w^0_1 = x_1, \quad w^0_2 = x_2.
$$

# Forward mode
Suppose we are interested in  $\partial f/\partial x_1$ and we want to fix $x_2$. The forward-mode corresponds to evaluating $\partial f/\partial x_1$ from bottom to top. We use $\dot{w} = \partial w/\partial x_1$ and work all the way up:

$$
\dot{w}_1^0 = 1, \quad \dot{w}_2^0 = 0; \\
\dot{w}_1^1 = \frac{\partial w^1_1}{\partial w_1^0}  \dot{w}_1^0 + \frac{\partial w^1_1}{\partial w_2^0}  \dot{w}_2^0, 
\quad \dot{w}_2^1 = \frac{\partial w^1_2}{\partial w_1^0}  \dot{w}_1^0 + \frac{\partial w^1_2}{\partial w_2^0}  \dot{w}_2^0 \\
\dot{w}^2 = \frac{\partial w^2}{\partial w_1^1}  \dot{w}_1^1 + \frac{\partial w^2}{\partial w_2^1}  \dot{w}_2^1
$$ 

Therefore, we need to keep track of the gradient of all operation units and build them up. Because many gradient forms involve the value of the variable, e.g. $\partial (x_1 x_2)/\partial x_1 = x_2$, we also need to keep track of the values of the operation units. 

We we need the derivative with respect to $x_2$ we set $\dot{w}_1^0 = 0, \dot{w}_2^0 = 1$ and redo the above. Therefore, the forward-mode requires $\mathcal{O}(N M)$ operations to evaluate the full derivative, where $N$ is the number of variables, and $M$ is the number of operation units. 

## Implementation
We use a class to keep track of the value of $w_i^n$ and the gradient $\dot{w}_i^n$, and we have to define the operation units, because now the operations not only returns the value, but also the gradient. A vanilla python implementation is provided in the following:

```python
'''
Forward-mode automatic differentiation for the function:
    f = x1*x2 + sin(x1) 
'''
import numpy as np

class fw_var():
    # define a new data structure to store both value and gradient
    def __init__(self, v=0, g=1):
        self.val = v # value
        self.grad = g # gradient
```
Next we define the operation units. 

```python

def f_prod(x, y):
    # evaluate x * y
    var = fw_var()
    var.val = x.val * y.val
    var.grad = x.val * y.grad + x.grad * y.val
    return var

def f_add(x, y):
    # evaluate x + y
    var = fw_var()
    var.val = x.val + y.val
    var.grad = x.grad + y.grad
    return var
```
And finally we build $f(x_1, x_2)$ and evaluate $\partial f/\partial x_1$.
```python
def func(x1, x2):
    # The function to be evaluated 
    # f = x1*x2 + sin(x1) 
    return f_add(f_prod(x1, x2), f_sin(x1))

x1 = fw_var(0, 1)
x2 = fw_var(1, 0)

var = func(x1, x2)
print(f"Value of f: {var.val}")
print(f"Grad wrt x1: {var.grad}")
```

One can also redefine the `+`, `-`, `*` and `/` signs by adding `__add__()`, `__sub__`, `__mul__()` and `__truediv__()` members to the class `fw_var`. You can find many fancier versions online :) 

