---
title: 'Automatic Differentiation'
date: 2023-08-17
permalink: /posts/2023/0817
tags:
  - Neural Networks
  - Machine Learning
  - Python3
---

This post explains how forward- and reverse-mode automatic differentiation algorithms work with the Wikipedia example $f(x_1, x_2) = x_1 * x_2 + \sin(x_1)$ and Python implementations. Autodiff is conceptually easy to understand, but untrivial to implement. Therefore, unless you can write your own autodiff code, you cannot say confidently that you understand how it works. 

If you have some basic understanding of autodiff, you can skip my writing and jump to the implementation.

The first step is to divide $f$ into operation units labeled by $w^{n}_i$, where $n$ stands for hierarchy, and $i$ stands for the index of the members in the same hierarchy. The lowest hierarcy corresponds to the variables, i.e., $x_1$ and $x_2$, and the highest hierarchy corresponds to the function, i.e., $f$. Therefore, we have

$$
w^2 = w^1_1 + w^1_2;\\
w^1_1 = w^0_1 * w^0_2, w^1_2 = \sin(w^0_1); \\
w^0_1 = x_1, w^0_2 = x_2.
$$

### Forward mode
Suppose we are interested in  $\partial f/\partial x_1$ and we want to fix $x_2$. The forward-mode corresponds to evaluating $\partial f/\partial x_1$ from bottom to top. We use $\dot{w} = \partial w/\partial x_1$ and work all the way up:

$$
\dot{w}_1^0 = 1, \dot{w}_2^0 = 0; \\
\dot{w}_1^1 = \frac{\partial w^1_1}{\partial w_1^0}  \dot{w}_1^0 + \frac{\partial w^1_1}{\partial w_2^0}  \dot{w}_2^0, \dot{w}_2^1 = \frac{\partial w^1_2}{\partial w_1^0}  \dot{w}_1^0 + \frac{\partial w^1_2}{\partial w_2^0}  \dot{w}_2^0 \\
\dot{w}^2 = \frac{\partial w^2}{\partial w_1^1}  \dot{w}_1^1 + \frac{\partial w^2}{\partial w_2^1}  \dot{w}_2^1
$$

Therefore, we need to keep track of the gradient of all operation units and build them up. Because many gradient forms involve the value of the variable, e.g. $\partial (x_1 x_2)/\partial x_1 = x_2$, we also need to keep track of the values of the operation units. 

We we need the derivative with respect to $x_2$ we set $\dot{w}_1^0 = 0, \dot{w}_2^0 = 1$ and redo the above. Therefore, the forward-mode requires $\mathcal{O} N M$ operations to evaluate the full derivative, where $N$ is the number of variables, and $M$ is the number of operation units. 

#### Implementation
We use a class to keep track of the value of $w_i^n$ and the gradient $\dot{w}_i^n$, and we can write a vanilla python implementation as following:


You can find fancier versions online, but I think the above is easiest to understand.

